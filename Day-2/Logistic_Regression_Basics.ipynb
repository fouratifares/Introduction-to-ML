{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrXxnldCUmV4"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the required libraries (needed when running outside colab where the environment doesn't come pre-loaded with libraries)\n",
        "\n",
        "%pip install numpy\n",
        "%pip install scikit-learn\n",
        "%pip install matplotlib\n",
        "%pip install tqdm\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "NjkKB8f-UvD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from copy import deepcopy\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "metadata": {
        "id": "Y3p224jCZANd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Contents:\n",
        "\n",
        "1. Scratch implementation of Logistic Regression using numpy on custom data\n",
        "2. Sklearn implementation of Logistic Regression on IRIS dataset\n",
        "\n",
        "\n",
        "You need to know:\n",
        "\n",
        "1. **numpy** (for impelementation)\n",
        "2. a little bit of **matplotlib** (for visualization)\n",
        "\n",
        "\n",
        "Good to have knowledge of:\n",
        "\n",
        "1. Sklearn (details of the functions is given anyways)"
      ],
      "metadata": {
        "id": "NuqXa4fvYBE7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing Logistic Regression from Scratch"
      ],
      "metadata": {
        "id": "ztlkgcooZj9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's build a custom dataset for our logistic regression model\n",
        "\n",
        "n_points = 400  # num of points in generated data\n",
        "max_coordinate_value = 100  # max possible value taken by the x or y coordinate of a point\n",
        "\n",
        "points = np.random.rand(n_points, 2) * max_coordinate_value\n",
        "\n",
        "l = np.sin(2*math.pi*(points[:, 0]/max_coordinate_value))\n",
        "l = (l*0.3*max_coordinate_value)+(max_coordinate_value/2)\n",
        "mask = points[:, 1] > l\n",
        "\n",
        "labels = np.zeros(n_points)\n",
        "labels[mask] = 1\n",
        "\n",
        "divider = (30 * np.sin(2*math.pi*np.linspace(0, max_coordinate_value, max_coordinate_value)/max_coordinate_value))\n",
        "divider = max_coordinate_value/2 + divider\n",
        "\n",
        "plt.plot(points[mask][:, 0], points[mask][:, 1], 'ro', label='1')\n",
        "plt.plot(points[~mask][:, 0], points[~mask][:, 1], 'bo', label='0')\n",
        "plt.plot(divider)  # visualization only\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ljO9jk91dTK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we get to coding, Here's are the equations for logistic regression cost function (cross-entropy) and sigmoid function\n",
        "\n",
        "Cost function:\n",
        "\n",
        "$$\n",
        "Cross-Entropy-Loss = -\\left( y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y}) \\right)\n",
        "$$\n",
        "\n",
        "Sigmoid:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$"
      ],
      "metadata": {
        "id": "1h7gzReVbKZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Apply sigmoid function. Sigmoid brings the output in 0 to 1 range\n",
        "    \"\"\"\n",
        "    return ?\n",
        "\n",
        "\n",
        "def log_reg_cost(y, y_pred):  # cross entropy\n",
        "    \"\"\"\n",
        "    Calculates and returns the cost for logistic regression.\n",
        "\n",
        "    Function is slightly different from mentioned above because this one handles batched/vector/multiple inputs (rather than just one).\n",
        "    This function sums and takes average across the vector.\n",
        "    \"\"\"\n",
        "\n",
        "    return ?\n",
        "\n",
        "\n",
        "def log_reg_gradient_descent(X, y, learning_rate, n_iters=500):\n",
        "    \"\"\"\n",
        "    Runs gradient descent (param optimization) for logistic regression and returns optimized weights.\n",
        "    \"\"\"\n",
        "    m, n = X.shape\n",
        "    theta = np.zeros(n)\n",
        "\n",
        "    losses = []\n",
        "\n",
        "    for _ in tqdm(range(n_iters)):\n",
        "        z = ?\n",
        "        y_pred = ?\n",
        "        gradient = ?\n",
        "        theta -= ?\n",
        "\n",
        "        loss = ?\n",
        "        losses.append(loss)\n",
        "\n",
        "    plt.plot(losses)\n",
        "\n",
        "    return theta"
      ],
      "metadata": {
        "id": "2r5cKzNMFJCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rename to X and y.\n",
        "\n",
        "X = points\n",
        "y = labels\n",
        "\n",
        "print(f'Current X shape: {X.shape}')\n",
        "\n",
        "intercept_col = np.ones(X.shape[0])  # a column of 1's for intercept value\n",
        "X = np.column_stack((X, intercept_col))\n",
        "\n",
        "print(f'New X shape: {X.shape}. Extra col added for intercept value')"
      ],
      "metadata": {
        "id": "wr5MMaz3eDRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Optimization\n",
        "\n",
        "num_iters = 30000\n",
        "lr = 1e-3\n",
        "\n",
        "theta = log_reg_gradient_descent(X, y, lr, num_iters)"
      ],
      "metadata": {
        "id": "ABm_5YOMrRgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "theta"
      ],
      "metadata": {
        "id": "pjmrIjhltn83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = sigmoid(np.dot(X, theta))  # output of sigmoid is in range 0-1\n",
        "y_pred = np.round(y_pred)  # round to 0 or 1\n",
        "\n",
        "correct_mask = (y_pred == y)\n",
        "correct_count = correct_mask.sum()\n",
        "total_count = len(y_pred)\n",
        "\n",
        "print(f'The model predicted {correct_count} out of {total_count} predictions correctly')\n",
        "print(f'The Accuracy is {correct_count/total_count} ({correct_count/total_count*100}%)')"
      ],
      "metadata": {
        "id": "J5lir5bOrHbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(X[correct_mask][:, 0], X[correct_mask][:, 1], 'go', label='Predicted correctly')\n",
        "plt.plot(X[~correct_mask][:, 0], X[~correct_mask][:, 1], 'ro', label='Predicted in-correctly')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JjCBoT1Yy_Pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we move on, let's also take a look at confusion matrix\n",
        "\n",
        "confusion matrix shows us the following:\n",
        "\n",
        "```\n",
        "TP FP\n",
        "FN TN\n",
        "```\n",
        "\n",
        "i.e\n",
        "\n",
        "- True Positive: How many actual labels were positive and were predicted positive too\n",
        "- False Positive: How many actual labels were negative but were predicted positive\n",
        "- False Negative: How many actual labels were positive but were predicted negative\n",
        "- True Negative: How many actual labels were negative and were predicted negative"
      ],
      "metadata": {
        "id": "RwXZL9jFjG7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(y, y_pred)"
      ],
      "metadata": {
        "id": "jQGK80gxjNt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sklearn implementation\n",
        "\n",
        "Also, let's try it on a famous dataset\n",
        "\n",
        "We'll work with **Iris** dataset.\n",
        "\n",
        "### About Iris\n",
        "The dataset is about classfying flowers based on length of their parts.\n",
        "There are more than 3 classes in it but we'll just pick a class and predict wether a flower IS that class or ISN'T\n",
        "\n",
        "### train test split\n",
        "We'll also seperate the test data out of training data, a common practice in when training models to see their performance"
      ],
      "metadata": {
        "id": "6Qn31b6zymHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's load iris and have a look at it\n",
        "\n",
        "iris_data = load_iris()  # returns a dict\n",
        "print(iris_data.keys(), '\\n')\n",
        "\n",
        "print('given features:', iris_data['feature_names'])\n",
        "print('flower names (class names):', iris_data['target_names'])"
      ],
      "metadata": {
        "id": "NADaK7QKxr5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Those are the given details. We're  concerned about 'data' (containing features of flowers) and target (containing classes)\n",
        "\n",
        "X = np.array(iris_data['data'])\n",
        "y = np.array(iris_data['target'])\n",
        "\n",
        "# Data is sorted by y. Let's shake things up a bit\n",
        "\n",
        "shuffled_idxs = list(range(0, len(y)))\n",
        "np.random.shuffle(shuffled_idxs)\n",
        "\n",
        "X = X[?]\n",
        "y = ?"
      ],
      "metadata": {
        "id": "OnPX7FuB1eMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "ZA0TCZxV5TFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape)\n",
        "print(y.shape)\n",
        "\n",
        "print(np.unique(y))"
      ],
      "metadata": {
        "id": "oOBF0SsG23aL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The next step after getting your hands on a dataset might normally be EDA, but we'll skip that.\n",
        "# we'll make a logistic regression model for class '0' (named setosa). The model returns 1 for flower IS setosa and 0 for flower is NOT setosa\n",
        "\n",
        "y[y==0]  = -1  # placeholder value\n",
        "y[y>0]   =  0\n",
        "y[y==-1] =  1  # Setosa labels are now 1 in 'y' and other flower's are 0"
      ],
      "metadata": {
        "id": "wkN_5zsp3BL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y  # 0 is replaced with 1 and non-0 are now 0"
      ],
      "metadata": {
        "id": "ao3NhAzN5Ucm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation of train_test_split\n",
        "\n",
        "The function is used to seperate out train and test data so we can train on different data and evaluate on different data.\n",
        "\n",
        "- Stratify: the shuffle will be made in such a way that the column specified in stratify will have equal distribution of different values in train and test splits.\n",
        "In our case, 1/3 of total data is 1 and 2/3 of data is 0. Stratifying by y means that this will be the case for y_train and y_test as well.\n",
        "- random_state: normally, split are made randomly but specifying a state means this cell will make the same split everytime.\n",
        "- train_size: how much of the data (out of 1) is kept as training. 0.7 means 70% data goes to training and 30% goes to testing"
      ],
      "metadata": {
        "id": "yiaDVFhC-Ukc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(?, ?, stratify=y, random_state=123, train_size=?)"
      ],
      "metadata": {
        "id": "G8v1fhNH-DcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression()\n",
        "model.fit(?, ?)  # we train on training data only"
      ],
      "metadata": {
        "id": "5yzIzmqs5UtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(?)  # to check the performance, we use the testing data split"
      ],
      "metadata": {
        "id": "2dBML2Fh5tVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct_mask = (y_pred == ?)  # the predictions from testing features is supposed to be compared to labels of testing data\n",
        "correct_count = correct_mask.sum()\n",
        "total_count = len(y_pred)\n",
        "\n",
        "print(f'The model predicted {correct_count} out of {total_count} predictions')\n",
        "print(f'The Accuracy is {correct_count/total_count} ({correct_count/total_count*100}%)')"
      ],
      "metadata": {
        "id": "nB_91KQB50WQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(y_test, y_pred)"
      ],
      "metadata": {
        "id": "DWWM9dxDLvI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "END."
      ],
      "metadata": {
        "id": "Yh9B854fWTle"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DZndsJsHY66t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
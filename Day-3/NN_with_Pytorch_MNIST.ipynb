{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFs5q8XAox82"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKSL__IwozZs"
      },
      "outputs": [],
      "source": [
        "# Download the required libraries (needed when running outside colab where the environment doesn't come pre-loaded with libraries)\n",
        "\n",
        "%pip install torch\n",
        "%pip install matplotlib\n",
        "%pip install torchvision\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xziZ2Kj1pZrq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "\n",
        "from torchvision.transforms.functional import to_tensor\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tH9YmdSAW7i"
      },
      "source": [
        "#Contents:\n",
        "\n",
        "1. Implementation of 1-2 layer NN fin pytorch which classifies MNIST dataset\n",
        "\n",
        "About MNIST:\n",
        "\n",
        "the dataset consists of images of 28x28 size. The image each contains a handwritten digit from 0 to 9. Our model needs to take this image and classify it to the correct digit.\n",
        "\n",
        "\n",
        "You need to know:\n",
        "\n",
        "1. **pytorch** (for impelementation)\n",
        "2. a little bit of **matplotlib** (for visualization)\n",
        "\n",
        "\n",
        "Good to have knowledge of:\n",
        "\n",
        "1. torch dataset and dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTqeY0V6pV3T"
      },
      "outputs": [],
      "source": [
        "# MNIST function fetches the MNIST dataset. Without any transform param, the returned object is a Pillow image but we want to convert it to numerical form\n",
        "# that is to say, a numpy array/torch tensor\n",
        "\n",
        "# to_tensor is used to avoid errors when creating data loader later. we'll convert them to numpy arrays when the time comes\n",
        "train_data = MNIST(root='./datasets', train=True, download=True, transform=to_tensor)\n",
        "test_data  = ?\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWmnHXbSr9Al"
      },
      "outputs": [],
      "source": [
        "batch_size = ?\n",
        "\n",
        "# Dataloaders are used to easily create batches of data so we can perform batch gradient descent for faster learning\n",
        "train_loader = ?\n",
        "test_loader = ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aQrW8JC0HBo"
      },
      "source": [
        "## Models\n",
        "\n",
        "let's create the architecture of our models\n",
        "\n",
        "Instead of sigmoid activation, we'll use softmax activation\n",
        "\n",
        "The difference is:\n",
        "- sigmoid brings each value in the range 0-1\n",
        "- softmax takes a vector and changes the value into probabilities: i.e the sum of those values = 1. The highest value in the original vector retains the highest value in softmax output\n",
        "\n",
        "Here's the formula for softmax:\n",
        "\n",
        "$$\n",
        "\\text{Softmax}(x)_i = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXfUV5ZLzGKa"
      },
      "outputs": [],
      "source": [
        "class NN1Layer(nn.Module):\n",
        "\n",
        "  def __init__(self, num_inp, num_out):\n",
        "\n",
        "    super(NN1Layer, self).__init__()\n",
        "\n",
        "    self.layer_1 = nn.Linear(num_inp, num_out)\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    z = ?\n",
        "    a = ?\n",
        "\n",
        "    return a\n",
        "\n",
        "\n",
        "class NN2Layer(nn.Module):\n",
        "\n",
        "  def __init__(self, num_inp, num_hidden, num_out):\n",
        "\n",
        "    super(NN2Layer, self).__init__()\n",
        "\n",
        "    self.layer_1 = ?\n",
        "    self.layer_2 = ?\n",
        "\n",
        "    self.hidden_activation = nn.ReLU()  # We can change the hidden activation (activation in between layer 1 and 2) here\n",
        "    self.softmax = ?  # dim 0 is normally batch size, we don't want to apply softmax across batch size\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    z1 = ?\n",
        "    a1 = ?\n",
        "    z2 = ?\n",
        "    a2 = ?\n",
        "\n",
        "    return a2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u_e7ldrtHh1"
      },
      "source": [
        "## The main training loop, with batch gradient descent.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhSQZRw36s0k"
      },
      "source": [
        "Declare model, it's params, optimizers and criterion etc\n",
        "\n",
        "We'll also declare a device here. This will let us use GPU\n",
        "you can see how much difference a GPU makes by changing the device param to cpu and cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCMTZkue74nL"
      },
      "outputs": [],
      "source": [
        "num_epochs = ?\n",
        "lr = ?\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # checks if machine supports cuda and if it does, we use that, otherwise cpu\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "# model = NN1Layer(?, 10)\n",
        "model = NN2Layer(?, ?, ?)  # The 2 layer one is equivalent to the one we implemented in numpy\n",
        "\n",
        "optimizer = Adam(?, lr=?)\n",
        "criterion = ?  # multi-class\n",
        "\n",
        "model.to(device)  # we need to send all input tensors as well as our model to this device. by default they are on cpu\n",
        "\n",
        "print(f'Using device {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4r4KeFeG8mF4"
      },
      "source": [
        "Pre-train performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-qDqF6f8oIp"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "model.eval()\n",
        "correctly_labelled = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "  val_epoch_weighted_loss = 0\n",
        "\n",
        "  for val_batch_X, val_batch_y in test_loader:\n",
        "\n",
        "    val_batch_X = val_batch_X.view(-1, ?).to(device)\n",
        "    val_batch_y = val_batch_y.to(device)\n",
        "\n",
        "    val_batch_y_probs = model(?)\n",
        "\n",
        "    loss = criterion(?, ?)\n",
        "    val_epoch_weighted_loss += (len(val_batch_y)*loss.item())\n",
        "\n",
        "    val_batch_y_pred = val_batch_y_probs.argmax(dim=1)  # convert probailities to labels by picking the label (index) with the highest prob\n",
        "\n",
        "    correctly_labelled += (val_batch_y_pred == ?).sum().item()  # item converts tensor to float/int/list\n",
        "\n",
        "val_epoch_loss = val_epoch_weighted_loss/len(test_loader.dataset)\n",
        "val_losses.append(val_epoch_loss)\n",
        "\n",
        "print(f'val_loss={val_epoch_loss}. labelled {correctly_labelled}/{len(test_loader.dataset)} correctly ({correctly_labelled/len(test_loader.dataset)*100}% accuracy)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZzBFOA48Fb7"
      },
      "source": [
        "Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sy0ChPzH7_wx"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "for epoch_no in range(num_epochs):\n",
        "\n",
        "  model.train()  # convert to train model. This turns out train-specific layers in the model (if you dont know about them, an example of them is dropout. more on this later)\n",
        "\n",
        "  epoch_weighted_loss = 0\n",
        "\n",
        "  for batch_X, batch_y in ?:\n",
        "\n",
        "    batch_X = batch_X.view(-1, 28*28).to(device)  # convert to [N, 28*28] shape where N is batch_size\n",
        "    batch_y = batch_y.to(device)\n",
        "\n",
        "    batch_y_probs = model(?)  # outputs [N, 10] where each [:, 10] is probabilities for class (0-9)\n",
        "\n",
        "    loss = ?\n",
        "\n",
        "    optimizer.zero_grad()  # need to clear out gradients from previous batch\n",
        "    loss.backward()  # calculate new gradients\n",
        "    optimizer.step()  # update weights\n",
        "\n",
        "    epoch_weighted_loss += (len(batch_y)*loss.item())\n",
        "\n",
        "  epoch_loss = epoch_weighted_loss/len(train_loader.dataset)\n",
        "  train_losses.append(epoch_loss)    # add loss for tracking. we'll visualize the loss trajectory later\n",
        "\n",
        "\n",
        "  # validation time\n",
        "\n",
        "  model.eval()  # take model to evaluation mode. turn off train-only layers\n",
        "  correctly_labelled = 0\n",
        "\n",
        "  with torch.no_grad():  # this makes our model to NOT track gradients\n",
        "\n",
        "    val_epoch_weighted_loss = 0\n",
        "\n",
        "    for val_batch_X, val_batch_y in test_loader:\n",
        "\n",
        "      val_batch_X = val_batch_X.view(-1, ?).to(device)\n",
        "      val_batch_y = val_batch_y.to(device)\n",
        "\n",
        "      val_batch_y_probs = ?\n",
        "\n",
        "      loss = criterion(?, val_batch_y)\n",
        "      val_epoch_weighted_loss += (len(val_batch_y)*loss.item())\n",
        "\n",
        "      val_batch_y_pred = val_batch_y_probs.argmax(dim=1)  # convert probailities to labels by picking the label (index) with the highest prob\n",
        "\n",
        "      correctly_labelled += (? == val_batch_y).sum().item()  # item converts tensor to float/int/list\n",
        "\n",
        "  val_epoch_loss = val_epoch_weighted_loss/len(test_loader.dataset)\n",
        "  val_losses.append(val_epoch_loss)\n",
        "\n",
        "  print(f'Epoch: {epoch_no}, train_loss={epoch_loss}, val_loss={val_epoch_loss}. labelled {correctly_labelled}/{len(test_loader.dataset)} correctly ({correctly_labelled/len(test_loader.dataset)*100}% accuracy)')\n",
        "\n",
        "print(f'Training complete on device {device}. Change device variable and run again to see the difference.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvNZwZsd_4Nf"
      },
      "source": [
        "Loss trajectory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jtaq-nooqSj9"
      },
      "outputs": [],
      "source": [
        "plt.plot(train_losses, label='train loss')\n",
        "plt.plot(val_losses, label='val loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (Cross Entropy)')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8Jf5i7IAswc"
      },
      "source": [
        "visualizing shapes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOaxXyMg-ahW"
      },
      "outputs": [],
      "source": [
        "print(val_batch_X.shape, val_batch_y.shape)  # 784 is 28*28\n",
        "print(val_batch_y_probs.shape)  # inference from model\n",
        "print(val_batch_y_pred.shape)  # probabilities converted\n",
        "\n",
        "print(\"\\n\\nTo verify softmax converts inputs into probabilities (sum of which is 1), let's sum those probabilities and see if we get 1's: \\n\")\n",
        "print(f'{val_batch_y_probs.sum(1)=}')\n",
        "\n",
        "print(f'\\n{val_batch_y_probs.sum(1).shape=}')\n",
        "\n",
        "print('\\n\\nLets see argmax in action')\n",
        "print(\"Here's one of the input to argmax\\n\")\n",
        "test_idx = 15\n",
        "print(val_batch_y_probs[test_idx].cpu())  # .cpu() brings a tensor back to cpu device from any other it might be on (like cuda)\n",
        "print(\"\\n\\nHere's its output\")\n",
        "print(val_batch_y_pred[test_idx].cpu())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iw2G2akA5lB"
      },
      "source": [
        "Saving and loading the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hi61n3DJAtvp"
      },
      "outputs": [],
      "source": [
        "# you can load save the model's state dict like this\n",
        "torch.save(model.state_dict(), 'MNIST_classifier.pt')  # take a look at the storage section if you're on colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MC84w0WpBE2Q"
      },
      "outputs": [],
      "source": [
        "loaded_model = NN2Layer(28*28, 32, 10)  # just an empty model.\n",
        "loaded_model.eval().to(device)\n",
        "\n",
        "print('Before loading model')\n",
        "\n",
        "with torch.no_grad():\n",
        "  probs = loaded_model(val_batch_X)\n",
        "  preds = probs.argmax(dim=1)\n",
        "\n",
        "  print(f'{(preds==val_batch_y).sum()}/{len(preds)} correct')\n",
        "\n",
        "\n",
        "print('After loading model')\n",
        "loaded_model.load_state_dict(torch.load('MNIST_classifier.pt'))\n",
        "\n",
        "with torch.no_grad():\n",
        "  probs = loaded_model(val_batch_X)\n",
        "  preds = probs.argmax(dim=1)\n",
        "\n",
        "  print(f'{(preds==val_batch_y).sum()}/{len(preds)} correct')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTWzqt7SCQaz"
      },
      "source": [
        "END."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2n4cITtHB6Xa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
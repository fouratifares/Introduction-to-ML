{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NFs5q8XAox82"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HKSL__IwozZs"
      },
      "outputs": [],
      "source": [
        "# Download the required libraries (needed when running outside colab where the environment doesn't come pre-loaded with libraries)\n",
        "\n",
        "%pip install torch\n",
        "%pip install matplotlib\n",
        "%pip install torchvision\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xziZ2Kj1pZrq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "\n",
        "from torchvision.transforms.functional import to_tensor\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tH9YmdSAW7i"
      },
      "source": [
        "#Contents:\n",
        "\n",
        "1. Implementation of 2 layer NN with pytorch which classifies CIFAR dataset\n",
        "\n",
        "About CIFAR:\n",
        "\n",
        "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
        "\n",
        "\n",
        "You need to know:\n",
        "\n",
        "1. **pytorch** (for impelementation)\n",
        "2. a little bit of **matplotlib** (for visualization)\n",
        "\n",
        "\n",
        "Good to have knowledge of:\n",
        "\n",
        "1. torch dataset and dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZTqeY0V6pV3T"
      },
      "outputs": [],
      "source": [
        "# MNIST function fetches the MNIST dataset. Without any transform param, the returned object is a Pillow image but we want to convert it to numerical form\n",
        "# that is to say, a numpy array/torch tensor\n",
        "\n",
        "# to_tensor is used to avoid errors when creating data loader later. we'll convert them to numpy arrays when the time comes\n",
        "train_data = CIFAR10(root='./datasets', train=True, download=True, transform=to_tensor)\n",
        "test_data  = CIFAR10(root='./datasets', train=False, download=True, transform=to_tensor)\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWmnHXbSr9Al"
      },
      "outputs": [],
      "source": [
        "batch_size = ?\n",
        "\n",
        "# Dataloaders are used to easily create batches of data so we can perform batch gradient descent for faster learning\n",
        "train_loader = ?\n",
        "test_loader = ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aQrW8JC0HBo"
      },
      "source": [
        "## Models\n",
        "\n",
        "let's create the architecture of our models\n",
        "\n",
        "Instead of sigmoid activation, we'll use softmax activation\n",
        "\n",
        "The difference is:\n",
        "- sigmoid brings each value in the range 0-1\n",
        "- softmax takes a vector and changes the value into probabilities: i.e the sum of those values = 1. The highest value in the original vector retains the highest value in softmax output\n",
        "\n",
        "Here's the formula for softmax:\n",
        "\n",
        "$$\n",
        "\\text{Softmax}(x)_i = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NN2Layer(nn.Module):\n",
        "\n",
        "  def __init__(self, num_inp, num_hidden, num_out):\n",
        "\n",
        "    super(NN2Layer, self).__init__()\n",
        "\n",
        "    ?\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    return ?"
      ],
      "metadata": {
        "id": "TN5SNXj5leQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u_e7ldrtHh1"
      },
      "source": [
        "## The main training loop, with batch gradient descent.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhSQZRw36s0k"
      },
      "source": [
        "Declare model, it's params, optimizers and criterion etc\n",
        "\n",
        "We'll also declare a device here. This will let us use GPU\n",
        "you can see how much difference a GPU makes by changing the device param to cpu and cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2n4cITtHB6Xa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}